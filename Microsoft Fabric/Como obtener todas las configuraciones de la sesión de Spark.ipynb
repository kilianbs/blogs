{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.get(\"spark.driver.cores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sempy.fabric as fabric\n",
    "\n",
    "default_lakehouse_id    = 'No default lakehouse' if spark.conf.get(\"trident.lakehouse.id\") == '' else spark.conf.get(\"trident.lakehouse.id\")\n",
    "default_lakehouse_name  = 'No default lakehouse' if spark.conf.get(\"trident.lakehouse.name\") == '' else spark.conf.get(\"trident.lakehouse.name\")\n",
    "notebook_item_id        = spark.conf.get(\"trident.artifact.id\")\n",
    "notebook_item_name      = fabric.resolve_item_name(notebook_item_id)\n",
    "pool_executor_cores     = spark.sparkContext.getConf().get(\"spark.executor.cores\")\n",
    "pool_executor_memory    = spark.sparkContext.getConf().get(\"spark.executor.memory\")\n",
    "pool_min_executors      = spark.sparkContext.getConf().get(\"spark.dynamicAllocation.minExecutors\")\n",
    "pool_max_executors      = spark.sparkContext.getConf().get(\"spark.dynamicAllocation.maxExecutors\")\n",
    "pool_number_of_nodes    = len(str(sc._jsc.sc().getExecutorMemoryStatus().keys()).replace(\"Set(\",\"\").replace(\")\",\"\").split(\", \"))\n",
    "spark_app_name          = spark.conf.get(\"spark.app.name\")\n",
    "workspace_id            = spark.conf.get(\"trident.workspace.id\")\n",
    "workspace_name          = fabric.resolve_workspace_name(workspace_id)\n",
    "\n",
    "print(f'default_lakehouse_id:   {default_lakehouse_id}')\n",
    "print(f'default_lakehouse_name: {default_lakehouse_name}')\n",
    "print(f'notebook_item_id:       {notebook_item_id}')\n",
    "print(f'notebook_item_name:     {notebook_item_name}')\n",
    "print(f'spark_app_name:         {spark_app_name}')\n",
    "print(f'pool_executor_cores:    {pool_executor_cores}')\n",
    "print(f'pool_executor_memory:   {pool_executor_memory}')\n",
    "print(f'pool_min_executors:     {pool_min_executors}')\n",
    "print(f'pool_max_executors:     {pool_max_executors}')\n",
    "print(f'pool_number_of_nodes:   {pool_number_of_nodes}')\n",
    "print(f'workspace_id:           {workspace_id}')\n",
    "print(f'workspace_name:         {workspace_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"<spark.conf.name>\", value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
